\documentclass{article}
%\documentclass[draft]{article}
\usepackage[left=1in,right=1in,top=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[inline]{enumitem}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue]{hyperref}
\usepackage[english]{babel}
\usepackage[backend=biber,sorting=none]{biblatex}
\usepackage{hyperref}
\usepackage{url}
\usepackage{minted}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{chngcntr}
\usepackage{xcolor}
\setlength{\parindent}{0pt}
%\addbibresource{main.bib}
\renewcommand\thesubsection{\arabic{subsection}}

\newminted{python}{breaklines=true,
  breakafter=/,
  linenos,
  numbersep=5pt,
  frame=lines,
  framesep=2mm}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},
  commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\normalsize,
  breakatwhitespace=true,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbers=left,
  numbersep=5pt,
  showspaces=false,
  showstringspaces=false,
  showtabs=true
  tabsize=4
}
\lstset{style=mystyle}

\title{APS Data Processing}
\author{SRJ Likith}
\date{}

%\pagenumbering{gobble}
\begin{document}
\maketitle

\section{Steps}

\subsection{Read relevant files}

First, the files containing the data and information that we need are read into \texttt{pandas DataFrames} using the \texttt{read\_excel} and \texttt{read\_csv} commands as appropriate.

\begin{pythoncode}
import pandas as pd
legend_df=pd.read_excel('SAXSframevstest_final.xlsx')
meta_df=pd.read_csv('aclarke_jul21_exp_tracking.csv',low_memory=False)
\end{pythoncode}


\subsection{Cleaning the data}

The first cleaning step, and these are aimed mainly at the \texttt{metadata} file, is to strip column names of leading and trailing whitespace characters. Next, any columns that consist of all \texttt{null}, \texttt{None}, or \texttt{NaN} values are removed. Lastly, the \texttt{Date} column is converted from strings to \texttt{datetime} objects. This is all achieved by the \texttt{clean\_df} function and the helper function \texttt{dt\_formatter}.

\begin{pythoncode}
from datetime import datetime as dt
  
def dt_formatter(dt_str):
    return dt.strptime(dt_str, '%a %b %d %H:%M:%S %Y)

def clean_df(in_df):
    in_df.columns=in_df.columns.str.strip()
    for c in in_df.columns:
        if in_df[c].isnull().all():
            del in_df[c]
    in_df['Date']=in_df['Date'].apply(dt_formatter)
    return in_df

meta_df=clean_df(meta_df)
\end{pythoncode}

\newpage
\subsection{Adding calculated column(s)}

Based on the \texttt{saxs fnum} column, the \texttt{wax\_fnum} column is added, which is \texttt{saxs fnum}/10 rounded to the nearest integer.

\begin{pythoncode}
def wax_fnum(row):
    return round(int(row['saxs fnum'])/10)

meta_df['wax fnum']=meta_df.apply(wax_fnum,axis=1)
\end{pythoncode}

\subsection{Processing}

In order to achieve all the processing steps, we loop through (\texttt{for} loop) the \texttt{legend\_df} \texttt{DataFrame}. The \texttt{idx} variable contains the index/row number while the \texttt{row} variable takes the values of the rows themselves.

\begin{pythoncode}
for idx, row in lengend_df.iterrows():
\end{pythoncode}

Note that the most following lines are all contained within this \texttt{for} loop and so, are indented one level (4 spaces/1 tab).

For each row, the first thing to do is to read the \texttt{SAXS Frame start} and \texttt{SAXS Frame End}, and then apply the \texttt{buffer} value, i.e., the limits now become \texttt{SAXS Frame start-buffer\_frames} and \texttt{SAXS Frame end+buffer\_frames}. Note that the \texttt{buffer\_frames=100} line needs to be run just once, and so, should be placed before the start of the \texttt{for} loop.

\begin{pythoncode}
buffer_frames=100 #this line needs to be placed before the for loop starts
  fnum_lims=[int(i) for i in [row['SAXS Frame Start'],row['SAXS Frame End']]]
  fnum_lims=[el+((-1)**(idex+1)*buffer_frames) for idex,el in enumerate(fnum_lims)]
\end{pythoncode}

Next, the sample name is read from the \texttt{SampleName} column and stripped of leading and trailing whitespaces. The \texttt{fname} variable is meant for naming the files and directories, and since this shouldn't have any \texttt{/} characters, these are replaced by \texttt{\_} characters.

\begin{pythoncode}
  sample=row['SampleName'].strip()
  fname=sample.replace('/','_')
\end{pythoncode}

Then, a small subset/subsection of the \texttt{meta\_df} is chosen that consists of the relevant columns, and only those rows with \texttt{saxs fnum} that lie between the limits mentioned earlier. Also, the \texttt{MTS load (mm)} is renamed to \texttt{Force (N)}. Again, like the previous block of code, these lines are indented one level since they are placed within the \texttt{for} loop.

\begin{pythoncode}
  curr_meta=meta_df[['Date', 'MTS load (mm)', 'Furnace T1 (C)',
                     'saxs fnum', 'wax fnum', 'MTS crosshead (mm)']][
                     (meta_df['saxs fnum']>=fnum_lims[0]) &
                     (meta_df['saxs fnum']<=fnum_lims[1])]
  curr_meta.rename({'MTS load (mm)':'Force (N)'}, axis=1, inplace=True)
\end{pythoncode}

Next, for each row in \texttt{legend\_df}, a folder is created (if it doesn't already exist), and the \texttt{curr\_meta} \texttt{DataFrame} is written to that directory. Note that the \texttt{file\_ops} function is defined \emph{before} the loop. The variable \texttt{p} helps point to the directory where we want all the sections and their directories deposited. This folder is created if it doesn't already exist.

\begin{pythoncode}
from pathlib import Path
p=Path('./sections_dir_final')
p.mkdir(parents=True, exist_ok=True)
def file_ops(sample_name,dest_dir=p)
    target=dest_dir/sample_name
    if not target.exists():
        target.mkdir(parents=True,exist_ok=True)

#the following lines are contained within the for loop, and hence, indented
  file_ops(fname)
  curr_meta.to_csv(p/fname/(fname+'_metadata.csv'),sep=',', index=False)      
\end{pythoncode}

Finally, a small \texttt{readme} file is deposited in each of the sub-directories, containing the following information about \texttt{buffer\_frames}

\begin{pythoncode}
readme_str='buffer_frames=100 #number of frames added to the beginning and end of each section to act as a buffer for context i.e., each section, instead of consisting of SAXS Frame Start to SAXS Frame End, will consist of (SAXS Frame Start - buffer) to (SAXS Frame End + buffer)' #defined before the loop

#within the loop
  with open(p/fname/'readme.txt','w') as f:
      f.writelines(readme_str)
\end{pythoncode}

\newpage

\section{Appendix}

\lstinputlisting[language=Python,caption=\texttt{processor\_final.py}]{processor_final.py}


\end{document}
